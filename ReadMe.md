# âœï¸ Patent Drafting with Large Language Models 
We aim for a paradigm shift for patent writing by leveraging LLMs
to facilitate the tedious patent-filing process.  We specifically focus on abstracts and claims by employing a zero-shot and few-shot approaches.

:boom: We generate abstracts of patents by utilizing the first claim
of each corresponding patent as an input for CPC subclasses A61, H04, and G06, and vice-versa. All the patents used in this paper are granted by the United States Patent and Trademark Office [USPTO](https://www.uspto.gov)  and obtained  from [PatentsView](https://patentsview.org/download/data-download-tables).

![example](example.jpg)

## Data
ğŸ”„ Data processing steps can be found  [here](https://github.com/hhshomee/patent_drafting/tree/main/Data%20Prep).

ğŸ“„ Sample data can be found [here](https://github.com/hhshomee/patent_drafting/tree/main/Sample%20data). All the processed data has the following information as columns: patent_id, patent_abstract, patent_title, claim_text, cpc_subclass.

## Patent Generation 

We generate abstracts and claims of patents using models such as GPT-2, GPT-3.5, Llama2, and Gemini.

## Patent Tasks

Patent classification has been done using pre trained BERT model[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]([https://github.com/hhshomee/patent_drafting/blob/main/Task/classification.ipynb])

For ranking we used BERT embedding and jaccard coefficient.


## Evaluation

To evaluate the generated abstracts, we employed the following NLP measures: BERTScore, BLEU, Rouge, and cosine similarity.


## Table: Evaluation of the generated abstracts by the NLP-based measures for the sub-classes in the **A61 (medical)** class

The models used here are Llama 2, GPT-2, and GPT-3.5. BERT-based metrics are constantly high across all subclasses. This demonstrates a strong performance of LLMs (best across models and measures are in green) in generating similar abstracts as the original ones. Llama 2 produces the best results (most number of green cells).

### Evaluation

To evaluate the generated abstracts, we employed the following NLP measures: BERTScore, BLEU, Rouge, and cosine similarity.

### Table: Evaluation of the generated abstracts by the NLP-based measures for the sub-classes in the A61 (medical) class

The models used here are Llama 2, GPT-2, and GPT-3.5. BERT-based metrics are constantly high across all subclasses. This demonstrates a strong performance of LLMs (best across models and measures are in green) in generating similar abstracts as the original ones. Llama 2 produces the best results (most number of green cells).

| Model  | CPC  | bert_F1             | cosine             | rouge_F1            | bleu_score          |
|--------|------|---------------------|--------------------|---------------------|---------------------|
| Llama 2| A61B | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.88 Â± 0.03 | 0.64 Â± 0.17       | 0.43 Â± 0.14         | 0.17 Â± 0.13         |
|        | A61F | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.88 Â± 0.03 | 0.60 Â± 0.19       | 0.39 Â± 0.14         | 0.14 Â± 0.12         |
|        | A61K | 0.85 Â± 0.03         | 0.39 Â± 0.17        | 0.28 Â± 0.13         | 0.07 Â± 0.08         |
|        | A61L | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.87 Â± 0.03 | 0.54 Â± 0.19       | 0.37 Â± 0.15         | 0.12 Â± 0.12         |
|        | A61M | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.88 Â± 0.03 | 0.62 Â± 0.19       | 0.41 Â± 0.15         | 0.16 Â± 0.13         |
|        | A61N | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.88 Â± 0.03 | 0.59 Â± 0.18       | 0.40 Â± 0.14         | 0.15 Â± 0.13         |
|        | A61P | 0.85 Â± 0.04         | 0.35 Â± 0.16        | 0.26 Â± 0.12         | 0.06 Â± 0.07         |
|        | A61Q | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.87 Â± 0.03 | 0.47 Â± 0.14       | 0.35 Â± 0.14         | 0.12 Â± 0.11         |
| GPT-2  | A61B | 0.82 Â± 0.03         | 0.35 Â± 0.17        | 0.20 Â± 0.10         | 0.03 Â± 0.06         |
|        | A61F | 0.81 Â± 0.02         | 0.31 Â± 0.17        | 0.18 Â± 0.09         | 0.02 Â± 0.02         |
|        | A61K | 0.81 Â± 0.03         | 0.32 Â± 0.14        | 0.18 Â± 0.09         | 0.02 Â± 0.04         |
|        | A61L | 0.81 Â± 0.02         | 0.31 Â± 0.14        | 0.19 Â± 0.10         | 0.02 Â± 0.05         |
|        | A61M | 0.82 Â± 0.03         | 0.38 Â± 0.17        | 0.22 Â± 0.12         | 0.04 Â± 0.09         |
|        | A61N | 0.82 Â± 0.03         | 0.38 Â± 0.16        | 0.22 Â± 0.10         | 0.02 Â± 0.04         |
|        | A61P | 0.80 Â± 0.02         | 0.27 Â± 0.15        | 0.15 Â± 0.07         | 0.01 Â± 0.03         |
|        | A61Q | 0.81 Â± 0.03         | 0.32 Â± 0.18        | 0.18 Â± 0.11         | 0.03 Â± 0.06         |
| GPT-3.5| A61B | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.88 Â± 0.02 | 0.59 Â± 0.14       | 0.39 Â± 0.10         | 0.12 Â± 0.08         |
|        | A61F | 0.87 Â± 0.02         | 0.52 Â± 0.18        | 0.33 Â± 0.11         | 0.07 Â± 0.07         |
|        | A61K | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.87 Â± 0.03 | 0.46 Â± 0.18       | 0.35 Â± 0.13         | 0.10 Â± 0.10         |
|        | A61L | 0.86 Â± 0.02         | 0.48 Â± 0.14        | 0.30 Â± 0.10         | 0.06 Â± 0.06         |
|        | A61M | 0.87 Â± 0.02         | 0.50 Â± 0.19        | 0.30 Â± 0.10         | 0.06 Â± 0.06         |
|        | A61N | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.88 Â± 0.03 | 0.56 Â± 0.16       | 0.38 Â± 0.11         | 0.11 Â± 0.08         |
|        | A61P | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.86 Â± 0.03 | 0.37 Â± 0.17       | 0.30 Â± 0.13         | 0.06 Â± 0.07         |
|        | A61Q | ![#20c997](https://via.placeholder.com/15/20c997/000000?text=+) 0.87 Â± 0.03 | 0.46 Â± 0.20       | 0.33 Â± 0.15         | 0.11 Â± 0.11         |
| Gemini | A61B | 0.87 Â± 0.05         | 0.56 Â± 0.27        | 0.39 Â± 0.21         | 0.16 Â± 0.21         |
|        | A61F | 0.86 Â± 0.04         | 0.54 Â± 0.15        | 0.37 Â± 0.19         | 0.14 Â± 0.15         |
|        | A61K | 0.85 Â± 0.04         | 0.37 Â± 0.19        | 0.28 Â± 0.16         | 0.08 Â± 0.11         |
|        | A61L | 0.86 Â± 0.04         | 0.50 Â± 0.23        | 0.35 Â± 0.18         | 0.12 Â± 0.13         |
|        | A61M | 0.87 Â± 0.04         | 0.54 Â± 0.26        | 0.38 Â± 0.20         | 0.13 Â± 0.14         |
|        | A61N | 0.88 Â± 0.04         | 0.58 Â± 0.22        | 0.42 Â± 0.16         | 0.16 Â± 0.13         |
|        | A61P | 0.85 Â± 0.06         | 0.36 Â± 0.17        | 0.26 Â± 0.14         | 0.06 Â± 0.10         |
|        | A61Q | 0.86 Â± 0.04         | 0.44 Â± 0.21        | 0.34 Â± 0.18         | 0.12 Â± 0.13         |
